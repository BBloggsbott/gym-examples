{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "seed = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10fc5e0b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(4, 128)\n",
    "        self.dropout = nn.Dropout(p=0.6)\n",
    "        self.affine2 = nn.Linear(128,2)\n",
    "        \n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.affine1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        action_scores = self.affine2(x)\n",
    "        return F.softmax(action_scores, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr = 1e-2)\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = policy(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    policy.saved_log_probs.append(m.log_prob(action))\n",
    "    return action.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_episode():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    returns = []\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "    for log_prob, R in zip(policy.saved_log_probs, returns):\n",
    "        policy_loss.append(-log_prob*R)\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tLast reward: 130.00\tAverage reward: 36.77 Saved Model.\n",
      "Episode 20\tLast reward: 101.00\tAverage reward: 54.93 Saved Model.\n",
      "Episode 30\tLast reward: 54.00\tAverage reward: 63.65 Saved Model.\n",
      "Episode 40\tLast reward: 110.00\tAverage reward: 69.10 Saved Model.\n",
      "Episode 50\tLast reward: 249.00\tAverage reward: 86.19 Saved Model.\n",
      "Episode 60\tLast reward: 108.00\tAverage reward: 105.47 Saved Model.\n",
      "Episode 70\tLast reward: 83.00\tAverage reward: 109.30 Saved Model.\n",
      "Episode 80\tLast reward: 173.00\tAverage reward: 123.21 Saved Model.\n",
      "Episode 90\tLast reward: 67.00\tAverage reward: 126.96 Saved Model.\n",
      "Episode 100\tLast reward: 23.00\tAverage reward: 95.29 \n",
      "Episode 110\tLast reward: 130.00\tAverage reward: 93.08 \n",
      "Episode 120\tLast reward: 500.00\tAverage reward: 161.34 Saved Model.\n",
      "Episode 130\tLast reward: 151.00\tAverage reward: 186.72 Saved Model.\n",
      "Episode 140\tLast reward: 155.00\tAverage reward: 172.95 \n",
      "Episode 150\tLast reward: 500.00\tAverage reward: 254.92 Saved Model.\n",
      "Episode 160\tLast reward: 416.00\tAverage reward: 297.21 Saved Model.\n",
      "Episode 170\tLast reward: 273.00\tAverage reward: 306.93 Saved Model.\n",
      "Episode 180\tLast reward: 500.00\tAverage reward: 334.44 Saved Model.\n",
      "Episode 190\tLast reward: 168.00\tAverage reward: 304.03 \n",
      "Episode 200\tLast reward: 183.00\tAverage reward: 271.77 \n",
      "Episode 210\tLast reward: 230.00\tAverage reward: 239.92 \n",
      "Episode 220\tLast reward: 180.00\tAverage reward: 227.16 Average Reward not increased for 40 episodes. Quitting.\n"
     ]
    }
   ],
   "source": [
    "running_reward = 10\n",
    "prev_running_reward = 0\n",
    "saved_at = 10\n",
    "for i_episode in count(1):\n",
    "    state, ep_reward = env.reset(), 0\n",
    "    for t in range(1, 1000):\n",
    "        action = select_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        env.render()\n",
    "        policy.rewards.append(reward)\n",
    "        ep_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    env.close()\n",
    "    running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "    finish_episode()\n",
    "    if i_episode % 10 == 0:\n",
    "        print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, running_reward), end=' ')\n",
    "        if running_reward > prev_running_reward or i_episode == 10:\n",
    "            torch.save(policy, \"cartpole_policy.pth\")\n",
    "            saved_at = i_episode\n",
    "            print('Saved Model.')\n",
    "        elif i_episode - saved_at > 30:\n",
    "            print('Average Reward not increased for {} episodes. Quitting.'.format(i_episode - saved_at))\n",
    "            break\n",
    "        else:\n",
    "            print()\n",
    "        prev_running_reward = running_reward\n",
    "    if running_reward > env.spec.reward_threshold:\n",
    "        print(\"Solved! Running reward is now {} and \"\n",
    "              \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
